{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52d97543-d741-4f7f-ad9d-c83bc062c2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "import pyarrow.parquet as pa\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, MinMaxScaler\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster, cophenet\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "TEST = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3ca29f1-109b-43d8-a703-06802756c55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in user history, i.e. dictionary containing all histories per user {<user_id>: {<article_id>, ...}}\n",
    "user_history_path = \"../DATA/user_history_selection/histories_pickle.txt\"\n",
    "user_history = pickle.load(open(user_history_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "688c5feb-376b-4a0b-8659-70be56673a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST PRINT\n",
    "if TEST:\n",
    "    K = 5\n",
    "    res = dict(list(user_history.items())[0: K])\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe1da279-1924-48e1-835a-ac975b500441",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_articles_info_path = \"../DATA/user_history_selection/final_cluster_data.txt\"\n",
    "user_articles_info = pickle.load(open(user_articles_info_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "372122f0-94ec-42c9-a217-91c5f5061afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST PRINT\n",
    "if TEST:\n",
    "    K = 1\n",
    "    res = dict(list(user_articles_info.items())[0: K])\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "41c1e68c-c5dc-4ee6-8d4b-c20bb6c921c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19704 users\n"
     ]
    }
   ],
   "source": [
    "print(len(user_articles_info), \"users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1fe1476-ca24-4ab6-97b6-0c9469216332",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST:\n",
    "    example_user_id = '2444624'\n",
    "    print(user_articles_info[example_user_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f8742d-68ef-4cbf-b346-b62bf79066d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing user 260338:   2%|â–Ž            | 463/19704 [03:03<3:55:50,  1.36it/s]"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "import random\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "# Dictionary containing al the relevant clustered articles per user\n",
    "user_clustered_articles = defaultdict(lambda: {})\n",
    "\n",
    "# Handle data (i.e. articles) with missing feature values\n",
    "def handle_missing_values(df):\n",
    "    # Drop entries missing any value\n",
    "    return df.dropna(how='any')\n",
    "\n",
    "# Insert per user its cluster ids and corresponding articles\n",
    "def insert_user_history_clusters(user_id, df, data_index_to_article_id):\n",
    "    # Get all possible clusters\n",
    "    clusters = df.cluster.unique()\n",
    "\n",
    "    if TEST:\n",
    "        print(\"Clusters user\", user_id, \":\", clusters)\n",
    "    \n",
    "    # print(df.dtypes)\n",
    "\n",
    "    for c in clusters:\n",
    "        if TEST:\n",
    "            print(\"Cluster:\", c)\n",
    "        \n",
    "        # Get all articles belonging to cluster c\n",
    "        clustered_articles = df.loc[df['cluster'] == c].copy()\n",
    "        # Sort articles history by most recent publishing date\n",
    "        ca_history = clustered_articles.sort_values(by='time_published', ascending = False)\n",
    "        \n",
    "        if TEST:\n",
    "            display(ca_history)\n",
    "        \n",
    "        article_id_indices = list(ca_history.index)\n",
    "\n",
    "        if TEST:\n",
    "            print(\"Cluster article indices df:\", article_id_indices)\n",
    "            \n",
    "        # For each article in the history save the <article_id> as a key and the <time_published> as value\n",
    "        for idx in article_id_indices:\n",
    "            # print(idx)\n",
    "            article_id = data_index_to_article_id[idx]\n",
    "            timestamp = df.loc[[idx],['time_published']].values[0]\n",
    "\n",
    "            if TEST:\n",
    "                print(timestamp)\n",
    "\n",
    "            # When cluster does not yet exist for user\n",
    "            if c not in user_clustered_articles[user_id]:\n",
    "                user_clustered_articles[user_id][c] = OrderedDict()\n",
    "                \n",
    "            user_clustered_articles[user_id][c][article_id] = timestamp\n",
    "\n",
    "        if TEST:\n",
    "            for cluster, articles in user_clustered_articles[user_id].items():\n",
    "                print(\"Cluster\", cluster)\n",
    "                for article_id, timestamp in articles.items():\n",
    "                    print(\"Article id\", article_id, \", timestamp\", timestamp)\n",
    "    \n",
    "            print(user_clustered_articles[user_id])\n",
    "\n",
    "if TEST:\n",
    "    N = 1\n",
    "else:\n",
    "    N = len(user_articles_info)\n",
    "    \n",
    "pbar = tqdm(list(user_articles_info.keys())[:N])\n",
    "\n",
    "# Loop through all users\n",
    "for user_id in pbar:\n",
    "    # sleep(0.25)\n",
    "    pbar.set_description(\"Processing user %s\" % user_id)\n",
    "\n",
    "    # Sample data\n",
    "    data = user_articles_info[user_id]\n",
    "\n",
    "    data_index_to_article_id = data['index']\n",
    "\n",
    "    data = {'topics': data['topics'],\n",
    "            'sentiment': data['sentiment'],\n",
    "            'page_views': data['page_views'],\n",
    "           'time_published': data['time_published']}\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    df = handle_missing_values(df)\n",
    "\n",
    "    # Multi-Label Binarizer for 'topics'\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    # Normalizer for 'page_views'\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    topics_encoded = mlb.fit_transform(df['topics'])\n",
    "    page_views_normalized = scaler.fit_transform(df[['page_views']])\n",
    "    \n",
    "    # Convert 'time_published' to datetime and extract the hour\n",
    "    df['time_published'] = pd.to_datetime(df['time_published'], unit='s', format='%y-%m-%d %H:%M:%S')\n",
    "    df['hour'] = df['time_published'].dt.hour\n",
    "    \n",
    "    # Normalize the hour (0-23) to be between 0 and 1\n",
    "    hour_normalized = df['hour'] / 23.0\n",
    "    hour_normalized = hour_normalized.values.reshape(-1, 1)\n",
    "    \n",
    "    # Combine all features\n",
    "    features = np.hstack((topics_encoded, df[['sentiment']].values, page_views_normalized, hour_normalized))\n",
    "    \n",
    "    # Define a function to calculate Gower distance\n",
    "    def gower_distance(X):\n",
    "        individual_variable_distances = []\n",
    "        for col in range(X.shape[1]):\n",
    "            if np.issubdtype(X[:, col].dtype, np.number):\n",
    "                range_ = np.ptp(X[:, col])\n",
    "                if range_ == 0:\n",
    "                    range_ = 1  # avoid division by zero\n",
    "                individual_variable_distances.append(pdist(X[:, col].reshape(-1, 1), metric='euclidean') / range_)\n",
    "            else:\n",
    "                individual_variable_distances.append(pdist(X[:, col].reshape(-1, 1), metric='hamming'))\n",
    "        return np.sqrt(sum(individual_variable_distances))\n",
    "    \n",
    "    # Calculate Gower distance matrix\n",
    "    gower_dist_matrix = squareform(gower_distance(features))\n",
    "    \n",
    "    # Perform hierarchical clustering\n",
    "    Z = linkage(gower_dist_matrix, method='ward')\n",
    "    \n",
    "    # Calculate cophenetic distances\n",
    "    c, coph_dists = cophenet(Z, pdist(gower_dist_matrix))\n",
    "\n",
    "    if TEST:\n",
    "        print(\"Max cophenetic distance threshold =\", math.floor(max(coph_dists)))\n",
    "    \n",
    "        # Plot the dendrogram\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        dendrogram(Z)\n",
    "        plt.title('Hierarchical Clustering Dendrogram')\n",
    "        plt.xlabel('Sample index')\n",
    "        plt.ylabel('Distance')\n",
    "        plt.show()\n",
    "        \n",
    "    # Determine cluster assignments\n",
    "    max_d = math.floor(max(coph_dists))  # set this value based on the dendrogram\n",
    "    clusters = fcluster(Z, max_d, criterion='distance')\n",
    "    \n",
    "    df['cluster'] = clusters\n",
    "\n",
    "    if TEST:\n",
    "        display(df)\n",
    "\n",
    "    insert_user_history_clusters(user_id, df, data_index_to_article_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72320c97-7f78-4f72-a5d5-3d4878fd6e9b",
   "metadata": {},
   "source": [
    "## DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5f8163-ac14-4032-beb7-230be4774316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "data = {\n",
    "    'article_id': [1234, 13255, 223943, 221432],\n",
    "    'topics': [['politics', 'economy'], ['technology'], ['health', 'science'], ['sports']],\n",
    "    'sentiment': [-1, 0, 1, 0],\n",
    "    'page_views': [100, 200, 300, 150],\n",
    "    'time_published': [1609459200, 1609545600, 1609632000, 1609718400]  # Unix timestamps\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Multi-Label Binarizer for 'topics' \n",
    "mlb = MultiLabelBinarizer()\n",
    "topics_encoded = mlb.fit_transform(df['topics'])\n",
    "\n",
    "# Normalize 'page_views'\n",
    "scaler = MinMaxScaler()\n",
    "page_views_normalized = scaler.fit_transform(df[['page_views']])\n",
    "\n",
    "# Convert 'time_published' to datetime and extract the hour\n",
    "df['time_published'] = pd.to_datetime(df['time_published'], unit='s')\n",
    "df['hour'] = df['time_published'].dt.hour\n",
    "\n",
    "# Normalize the hour (0-23) to be between 0 and 1\n",
    "hour_normalized = df['hour'] / 23.0\n",
    "hour_normalized = hour_normalized.values.reshape(-1, 1)\n",
    "\n",
    "# Combine all features\n",
    "features = np.hstack((topics_encoded, df[['sentiment']].values, page_views_normalized, hour_normalized))\n",
    "\n",
    "# Define a function to calculate Gower distance\n",
    "def gower_distance(X):\n",
    "    individual_variable_distances = []\n",
    "    for col in range(X.shape[1]):\n",
    "        if np.issubdtype(X[:, col].dtype, np.number):\n",
    "            range_ = np.ptp(X[:, col])\n",
    "            if range_ == 0:\n",
    "                range_ = 1  # avoid division by zero\n",
    "            individual_variable_distances.append(pdist(X[:, col].reshape(-1, 1), metric='euclidean') / range_)\n",
    "        else:\n",
    "            individual_variable_distances.append(pdist(X[:, col].reshape(-1, 1), metric='hamming'))\n",
    "    return np.sqrt(sum(individual_variable_distances))\n",
    "\n",
    "# Calculate Gower distance matrix\n",
    "gower_dist_matrix = squareform(gower_distance(features))\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "Z = linkage(gower_dist_matrix, method='ward')\n",
    "\n",
    "# Plot the dendrogram\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(Z)\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Sample index')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n",
    "\n",
    "# Calculate cophenetic distances\n",
    "c, coph_dists = cophenet(Z, pdist(gower_dist_matrix))\n",
    "\n",
    "print(coph_dists)\n",
    "print(math.ceil(max(coph_dists)))\n",
    "print(math.floor(max(coph_dists)))\n",
    "\n",
    "# Determine cluster assignments\n",
    "max_d = 3  # set this value based on the dendrogram\n",
    "clusters = fcluster(Z, max_d, criterion='distance')\n",
    "\n",
    "df['cluster'] = clusters\n",
    "\n",
    "print(df)\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
