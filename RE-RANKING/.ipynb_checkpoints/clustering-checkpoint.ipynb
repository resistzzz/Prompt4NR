{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "52d97543-d741-4f7f-ad9d-c83bc062c2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "import pyarrow.parquet as pa\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, MinMaxScaler\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster, cophenet\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# This flag gives the possibility to run the code in testmode\n",
    "# i.e. test prints and limited I/O output such that it does not exceed the buffer (e.g. only printing one user) \n",
    "TEST = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d3ca29f1-109b-43d8-a703-06802756c55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in user history, i.e. dictionary containing all histories per user {<user_id>: {<article_id>, ...}}\n",
    "user_history_path = \"../DATA/user_history_selection/histories_pickle.txt\"\n",
    "user_history = pickle.load(open(user_history_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "688c5feb-376b-4a0b-8659-70be56673a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST PRINT\n",
    "if TEST:\n",
    "    K = 1\n",
    "    res = dict(list(user_history.items())[0: K])\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fe1da279-1924-48e1-835a-ac975b500441",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_articles_info_path = \"../DATA/user_history_selection/final_cluster_data.txt\"\n",
    "user_articles_info = pickle.load(open(user_articles_info_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "372122f0-94ec-42c9-a217-91c5f5061afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST PRINT\n",
    "if TEST:\n",
    "    K = 1\n",
    "    res = dict(list(user_articles_info.items())[0: K])\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "41c1e68c-c5dc-4ee6-8d4b-c20bb6c921c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19704 users\n"
     ]
    }
   ],
   "source": [
    "print(len(user_articles_info), \"users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d1fe1476-ca24-4ab6-97b6-0c9469216332",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST:\n",
    "    example_user_id = '1404234' #list(user_articles_info.keys())[0]\n",
    "    print(\"User\", example_user_id)\n",
    "    print(user_articles_info[example_user_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5873f36a-9c45-4bb4-ad85-fe89f514c5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On average, the average 'page_views' per user is 110880.13820090744\n"
     ]
    }
   ],
   "source": [
    "df_ua_info = pd.DataFrame(user_articles_info).T\n",
    "\n",
    "if TEST:\n",
    "    display(df_ua_info)\n",
    "\n",
    "# Compute what the average 'page_views' is for a user given the whole dataset, \n",
    "# i.e. the average 'page_views' for whole dataset of the average 'page_views' per user history\n",
    "count_user_avg_page_views = np.array([])\n",
    "for page_views in df_ua_info[\"page_views\"]:\n",
    "    user_avg_page_views = np.nanmean(np.array(page_views)) \n",
    "    count_user_avg_page_views = np.append(count_user_avg_page_views, user_avg_page_views)\n",
    "\n",
    "avg_avg_user_page_views = np.nanmean(count_user_avg_page_views)\n",
    "print(\"On average, the average 'page_views' per user is\", avg_avg_user_page_views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f8742d-68ef-4cbf-b346-b62bf79066d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing user 486637:   2%|‚ñè           | 326/19704 [13:26<13:46:53,  2.56s/it]"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "import random\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "# Dictionary containing al the relevant clustered articles per user\n",
    "user_clustered_articles = {}\n",
    "\n",
    "# Insert per user its cluster ids and corresponding articles\n",
    "def insert_user_history_clusters(user_id, df, data_index_to_article_id):\n",
    "    # Get all possible clusters\n",
    "    clusters = df.cluster.unique()\n",
    "\n",
    "    if TEST:\n",
    "        print(\"Clusters user\", user_id, \":\", clusters)\n",
    "    \n",
    "    # print(df.dtypes)\n",
    "\n",
    "    for c in clusters:\n",
    "        if TEST:\n",
    "            print(\"Cluster:\", c)\n",
    "        \n",
    "        # Get all articles belonging to cluster c\n",
    "        clustered_articles = df.loc[df['cluster'] == c].copy()\n",
    "        # Sort articles history by most recent publishing date\n",
    "        ca_history = clustered_articles.sort_values(by='time_published', ascending = False)\n",
    "        \n",
    "        if TEST:\n",
    "            display(ca_history)\n",
    "        \n",
    "        article_id_indices = list(ca_history.index)\n",
    "\n",
    "        if TEST:\n",
    "            print(\"Cluster article indices df:\", article_id_indices)\n",
    "            \n",
    "        # For each article in the history save the <article_id> as a key and the <time_published> as value\n",
    "        for idx in article_id_indices:\n",
    "            # print(idx)\n",
    "            article_id = data_index_to_article_id[idx]\n",
    "            timestamp = df.loc[[idx],['time_published']].values[0]\n",
    "\n",
    "            if TEST:\n",
    "                print(timestamp)\n",
    "\n",
    "            if user_id not in user_clustered_articles:\n",
    "                user_clustered_articles[user_id] = {}\n",
    "                \n",
    "            # When cluster does not yet exist for user\n",
    "            if c not in user_clustered_articles[user_id]:\n",
    "                user_clustered_articles[user_id][c] = OrderedDict()\n",
    "                \n",
    "            user_clustered_articles[user_id][c][article_id] = timestamp\n",
    "\n",
    "        if TEST:\n",
    "            for cluster, articles in user_clustered_articles[user_id].items():\n",
    "                print(\"Cluster\", cluster)\n",
    "                for article_id, timestamp in articles.items():\n",
    "                    print(\"Article id\", article_id, \", timestamp\", timestamp)\n",
    "    \n",
    "            print(user_clustered_articles[user_id])\n",
    "\n",
    "if TEST:\n",
    "    N = 1\n",
    "else:\n",
    "    N = len(user_articles_info)\n",
    "    \n",
    "pbar = tqdm(list(user_articles_info.keys())[:N])\n",
    "\n",
    "# Loop through all users\n",
    "for user_id in pbar:\n",
    "    if TEST:\n",
    "        user_id = example_user_id\n",
    "        \n",
    "    # sleep(0.25)\n",
    "    pbar.set_description(\"Processing user %s\" % user_id)\n",
    "\n",
    "    # Sample data\n",
    "    data = user_articles_info[user_id]\n",
    "\n",
    "    data_index_to_article_id = data['index']\n",
    "\n",
    "    data = {'topics': data['topics'],\n",
    "            'sentiment': data['sentiment'],\n",
    "            'page_views': data['page_views'],\n",
    "           'time_published': data['time_published']}\n",
    "\n",
    "    # Number of articles\n",
    "    n_samples = len(data['page_views'])\n",
    "    \n",
    "    if TEST:\n",
    "        print(\"Number of articles\", n_samples)\n",
    "        \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Calculate the average of these columns ignoring NaN values\n",
    "    avg = df[['sentiment', 'page_views', 'time_published']].mean(skipna=True)\n",
    "\n",
    "    # When average page view is nan take the user's average 'page_view' on average \n",
    "    # (this case only occurs when user has only one article for which this value is nan \n",
    "    if math.isnan(avg['page_views']):\n",
    "        avg['page_views'] = avg_avg_user_page_views\n",
    "\n",
    "    # Imputation by the mean\n",
    "    df = df.fillna({'sentiment': avg['sentiment'], 'page_views': avg['page_views'], 'time_published': avg['time_published']})\n",
    "\n",
    "    # User history containing only one article needs no clustering, hardcode data and continue\n",
    "    if len(df.index) == 1:\n",
    "        df['cluster'] = [1]\n",
    "        insert_user_history_clusters(user_id, df, data_index_to_article_id)\n",
    "        continue\n",
    "    \n",
    "    # Multi-Label Binarizer for 'topics'\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    # Normalizer for 'page_views'\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    topics_encoded = mlb.fit_transform(df['topics'])\n",
    "    page_views_normalized = scaler.fit_transform(df[['page_views']])\n",
    "    \n",
    "    # Convert 'time_published' to datetime and extract the hour\n",
    "    df['time_published'] = pd.to_datetime(df['time_published'], unit='s', format='%y-%m-%d %H:%M:%S')\n",
    "    df['hour'] = df['time_published'].dt.hour\n",
    "    \n",
    "    # Normalize the hour (0-23) to be between 0 and 1\n",
    "    hour_normalized = df['hour'] / 23.0\n",
    "    hour_normalized = hour_normalized.values.reshape(-1, 1)\n",
    "    \n",
    "    # Combine all features\n",
    "    features = np.hstack((topics_encoded, df[['sentiment']].values, page_views_normalized, hour_normalized))\n",
    "    \n",
    "    # Define a function to calculate Gower distance\n",
    "    def gower_distance(X):\n",
    "        individual_variable_distances = []\n",
    "        for col in range(X.shape[1]):\n",
    "            if np.issubdtype(X[:, col].dtype, np.number):\n",
    "                range_ = np.ptp(X[:, col])\n",
    "                if range_ == 0:\n",
    "                    range_ = 1  # avoid division by zero\n",
    "                individual_variable_distances.append(pdist(X[:, col].reshape(-1, 1), metric='euclidean') / range_)\n",
    "            else:\n",
    "                individual_variable_distances.append(pdist(X[:, col].reshape(-1, 1), metric='hamming'))\n",
    "        return np.sqrt(sum(individual_variable_distances))\n",
    "    \n",
    "    # Calculate Gower distance matrix\n",
    "    gower_dist_matrix = squareform(gower_distance(features))\n",
    "    \n",
    "    # Perform hierarchical clustering\n",
    "    Z = linkage(gower_dist_matrix, method='ward')\n",
    "    \n",
    "    # Calculate cophenetic distances\n",
    "    c, coph_dists = cophenet(Z, pdist(gower_dist_matrix))\n",
    "\n",
    "    if TEST:\n",
    "        \n",
    "        \n",
    "\n",
    "    if TEST:\n",
    "        print(\"cophenetic coefficient\", c)\n",
    "        print(\"cophenetic distances\", coph_dists)\n",
    "        print(\"Number of cophenetic distances\", len(coph_dists))\n",
    "        print(\"Min cophenetic distance threshold =\", math.floor(min(coph_dists)))\n",
    "        print(\"Max cophenetic distance threshold =\", math.floor(max(coph_dists)))\n",
    "\n",
    "        # Plot the dendrogram\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        dendrogram(Z)\n",
    "        plt.title('Hierarchical Clustering Dendrogram')\n",
    "        plt.xlabel('Sample index')\n",
    "        plt.ylabel('Distance')\n",
    "        plt.show()\n",
    "\n",
    "    # Determine the range of t values from the linkage matrix\n",
    "    min_distance = math.floor(min(coph_dists))\n",
    "    max_distance = math.floor(max(coph_dists))\n",
    "\n",
    "    # Extend the range slightly beyond the min and max distances\n",
    "    t_values = np.linspace(min_distance + 0.5, max_distance + 0.5, 100)\n",
    "\n",
    "    # Try different thresholds for parameter t and determine optimal value\n",
    "    silhouette_scores = []\n",
    "\n",
    "    for t in t_values:\n",
    "        clusters = fcluster(Z, t, criterion='distance')\n",
    "        num_clusters = len(set(clusters))\n",
    "\n",
    "        if TEST:\n",
    "            print(\"Trying value\", t)\n",
    "            print(num_clusters, clusters)\n",
    "        \n",
    "        if num_clusters > 1:  # Ensure there is more than one cluster\n",
    "            if num_clusters >=  n_samples:\n",
    "                silhouette_scores.append(-1) # Assign a low score for number of clusters not in range [2, n_samples - 1]\n",
    "            else:\n",
    "                score = silhouette_score(gower_dist_matrix, clusters)\n",
    "                silhouette_scores.append(score)\n",
    "        else:\n",
    "            silhouette_scores.append(-1)  # Assign a low score for a single cluster\n",
    "    \n",
    "    # Find the t value that maximizes the silhouette score\n",
    "    optimal_t = t_values[np.argmax(silhouette_scores)]\n",
    "\n",
    "    if TEST:\n",
    "        print(f\"Optimal t value: {optimal_t}\")\n",
    "        \n",
    "    # Determine cluster assignments\n",
    "    max_d = optimal_t  # This value could also be set based on the dendrogram\n",
    "    clusters = fcluster(Z, max_d, criterion='distance')\n",
    "    \n",
    "    df['cluster'] = clusters\n",
    "\n",
    "    if TEST:\n",
    "        display(df)\n",
    "\n",
    "    insert_user_history_clusters(user_id, df, data_index_to_article_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0f5c86-6f97-4deb-9f39-a073d6b13feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Write data: clustered user articles history\n",
    "# Structures :\n",
    "\"\"\"\n",
    "{<user_id>: \n",
    "    {<cluster_id>: \n",
    "        {<article_id>: <timestamp_published>,\n",
    "        <article_id>: <timestamp_published>,\n",
    "        ‚Ä¶,\n",
    "        <article_id>: <timestamp_published>}\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "with open('../DATA/user_history_selection/user_clustered_articles_history.pickle', 'wb') as handle:\n",
    "    pickle.dump(user_clustered_articles, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b655468-1524-405f-b98c-fccb99c143ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "with open('../DATA/user_history_selection/user_clustered_articles_history.pickle', 'rb') as handle:\n",
    "    file = pickle.load(handle)\n",
    "\n",
    "K = 1\n",
    "res = dict(list(file.items())[0: K])\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72320c97-7f78-4f72-a5d5-3d4878fd6e9b",
   "metadata": {},
   "source": [
    "## DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5f8163-ac14-4032-beb7-230be4774316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "data = {\n",
    "    'article_id': [1234, 13255, 223943, 221432],\n",
    "    'topics': [['politics', 'economy'], ['technology'], ['health', 'science'], ['sports']],\n",
    "    'sentiment': [-1, 0, 1, 0],\n",
    "    'page_views': [100, 200, 300, 150],\n",
    "    'time_published': [1609459200, 1609545600, 1609632000, 1609718400]  # Unix timestamps\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Multi-Label Binarizer for 'topics' \n",
    "mlb = MultiLabelBinarizer()\n",
    "topics_encoded = mlb.fit_transform(df['topics'])\n",
    "\n",
    "# Normalize 'page_views'\n",
    "scaler = MinMaxScaler()\n",
    "page_views_normalized = scaler.fit_transform(df[['page_views']])\n",
    "\n",
    "# Convert 'time_published' to datetime and extract the hour\n",
    "df['time_published'] = pd.to_datetime(df['time_published'], unit='s')\n",
    "df['hour'] = df['time_published'].dt.hour\n",
    "\n",
    "# Normalize the hour (0-23) to be between 0 and 1\n",
    "hour_normalized = df['hour'] / 23.0\n",
    "hour_normalized = hour_normalized.values.reshape(-1, 1)\n",
    "\n",
    "# Combine all features\n",
    "features = np.hstack((topics_encoded, df[['sentiment']].values, page_views_normalized, hour_normalized))\n",
    "\n",
    "# Define a function to calculate Gower distance\n",
    "def gower_distance(X):\n",
    "    individual_variable_distances = []\n",
    "    for col in range(X.shape[1]):\n",
    "        if np.issubdtype(X[:, col].dtype, np.number):\n",
    "            range_ = np.ptp(X[:, col])\n",
    "            if range_ == 0:\n",
    "                range_ = 1  # avoid division by zero\n",
    "            individual_variable_distances.append(pdist(X[:, col].reshape(-1, 1), metric='euclidean') / range_)\n",
    "        else:\n",
    "            individual_variable_distances.append(pdist(X[:, col].reshape(-1, 1), metric='hamming'))\n",
    "    return np.sqrt(sum(individual_variable_distances))\n",
    "\n",
    "# Calculate Gower distance matrix\n",
    "gower_dist_matrix = squareform(gower_distance(features))\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "Z = linkage(gower_dist_matrix, method='ward')\n",
    "\n",
    "# Plot the dendrogram\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(Z)\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Sample index')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n",
    "\n",
    "# Calculate cophenetic distances\n",
    "c, coph_dists = cophenet(Z, pdist(gower_dist_matrix))\n",
    "\n",
    "print(coph_dists)\n",
    "print(math.ceil(max(coph_dists)))\n",
    "print(math.floor(max(coph_dists)))\n",
    "\n",
    "# Determine cluster assignments\n",
    "max_d = 3  # set this value based on the dendrogram\n",
    "clusters = fcluster(Z, max_d, criterion='distance')\n",
    "\n",
    "df['cluster'] = clusters\n",
    "\n",
    "print(df)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155b3a15-2854-4e0b-8076-7dbcf035538e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
