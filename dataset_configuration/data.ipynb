{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This notebook shows how to retrieve the necessary data from the EB-NeRD data to create datasets with the correct structure for the Prompt4NR framework and our extensions.\n",
    "The data files themselves can easily be accessed from the README file!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5804ec7fe10420cf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load EB-NeRD files"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b64e7e1caf39c7d"
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-26T08:23:16.838921Z",
     "start_time": "2024-06-26T08:23:00.152521Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install pandas pyarrow needed for reading parquet files\n",
    "import pandas as pd\n",
    "\n",
    "articles_df = pd.read_parquet('data/ebnerd_large/articles.parquet')\n",
    "history_df = pd.read_parquet('data/ebnerd_large/train/history.parquet')\n",
    "behaviors_df = pd.read_parquet('data/ebnerd_large/train/behaviors.parquet')\n",
    "val_history_df = pd.read_parquet('data/ebnerd_large/validation/history.parquet')\n",
    "val_behaviors_df = pd.read_parquet('data/ebnerd_large/validation/behaviors.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Turning articles.parquet into news.txt file (dictionary) for all articles"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb3ab66140f9fec2"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import pickle\n",
    "def restructure_article(row):\n",
    "    return {\n",
    "        str(row['article_id']): {\n",
    "            'cate': str(row['category']),\n",
    "            'subcate': str(row['subcategory']),\n",
    "            'title': row['title'],\n",
    "            'abstract': row['subtitle'], \n",
    "            'sentiment': row['sentiment_label'],\n",
    "            'topics': row['topics']  \n",
    "        }\n",
    "    }\n",
    "\n",
    "new_dataset_dict = {}\n",
    "for _, row in articles_df.iterrows():\n",
    "    new_dataset_dict.update(restructure_article(row))\n",
    "\n",
    "with open('news.txt', 'wb') as handle:\n",
    "    pickle.dump(new_dataset_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T17:04:24.063265Z",
     "start_time": "2024-06-28T17:04:24.058265Z"
    }
   },
   "id": "53b5ee22150125f3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "note at this point that we also subsetted the news.txt dict to only include the articles actually used in the train/val and test files. But this is technically not necessary."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37080952401bf8e7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Stratified sampling based on gender and age"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1477fedced672dd3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def stratified_sample_df(df, gender_col, age_col, num_samples):\n",
    "    gender_distribution = df[gender_col].value_counts(normalize=True)\n",
    "    samples_per_gender = (gender_distribution * num_samples).round().astype(int)\n",
    "    \n",
    "    sample_df = pd.DataFrame()\n",
    "    for gender_value, n_samples in samples_per_gender.items():\n",
    "        gender_df = df[df[gender_col] == gender_value]\n",
    "        age_distribution = gender_df[age_col].value_counts(normalize=True)\n",
    "        if age_distribution.sum() == 0 or n_samples == 0:\n",
    "            continue\n",
    "        sampled_df = gender_df.sample(n=min(n_samples, len(gender_df)), weights=gender_df[age_col].map(age_distribution), replace=False)\n",
    "        sample_df = pd.concat([sample_df, sampled_df])\n",
    "    return sample_df\n",
    "\n",
    "#adjust the dataframe and number of samples to get the amount of data you want for train and val \n",
    "#e.g.\n",
    "train_stratified_sample = stratified_sample_df(behaviors_df, 'gender', 'age', 7849)\n",
    "val_stratified_sample = stratified_sample_df(val_behaviors_df, 'gender', 'age', 7849)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "209bebafcf8453cb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Turning history and behaviors parquet into train.txt, val.txt and test.txt files"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4c5c065099b399b"
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "def get_users_click_history(user_id, current_impression_time, cur_history_df):\n",
    "    #get histories until current impression (aka session click)\n",
    "    user_history = cur_history_df[cur_history_df['user_id'] == user_id]\n",
    "    past_interactions = []\n",
    "    for index, row in user_history.iterrows():\n",
    "        for art_id, imp_time in zip(row['article_id_fixed'], row['impression_time_fixed']):\n",
    "            if imp_time < current_impression_time:\n",
    "                past_interactions.append(str(art_id))\n",
    "            else:\n",
    "                print(\"nope\")\n",
    "    \n",
    "    return past_interactions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T07:38:01.547443Z",
     "start_time": "2024-06-18T07:38:01.521538Z"
    }
   },
   "id": "fcb34bfab5944eda"
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "new_train_data = [ [],[],[], [] ]\n",
    "\n",
    "for index, row in train_stratified_sample.iterrows():\n",
    "    new_train_data[0].append(row['impression_id'])\n",
    "    new_train_data[1].append(str(row['user_id']))\n",
    "    new_train_data[2].append(str(row['impression_time']))\n",
    "    \n",
    "    clicked_articles = set(row['article_ids_clicked']) if len(row['article_ids_clicked']) > 0 else set()\n",
    "    inview_articles = set(row['article_ids_inview']) if len(row['article_ids_inview']) > 0 else set()\n",
    "    positive_samples = [str(article) for article in clicked_articles] \n",
    "    negative_samples = [str(article) for article in (inview_articles - clicked_articles)]\n",
    "    click_history = get_users_click_history(row['user_id'], row['impression_time'],history_df)\n",
    "    new_train_data[3].append([\n",
    "        click_history,\n",
    "        positive_samples,\n",
    "        negative_samples\n",
    "    ])\n",
    "\n",
    "with open('train.txt', 'wb') as f:\n",
    "    pickle.dump(new_train_data, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T07:39:41.818156Z",
     "start_time": "2024-06-18T07:38:03.909044Z"
    }
   },
   "id": "76ebb81ced84935f"
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "new_val_data = [ [],[],[], [] ]\n",
    "\n",
    "for index, row in val_stratified_sample.iterrows():\n",
    "    new_val_data[0].append(row['impression_id'])\n",
    "    new_val_data[1].append(str(row['user_id']))\n",
    "    new_val_data[2].append(str(row['impression_time']))\n",
    "    \n",
    "    clicked_articles = set(row['article_ids_clicked']) if len(row['article_ids_clicked']) > 0 else set()\n",
    "    inview_articles = set(row['article_ids_inview']) if len(row['article_ids_inview']) > 0 else set()\n",
    "    positive_samples = [str(article) for article in clicked_articles] \n",
    "    negative_samples = [str(article) for article in (inview_articles - clicked_articles)]\n",
    "    click_history = get_users_click_history(row['user_id'], row['impression_time'], val_history_df)\n",
    "    new_val_data[3].append([\n",
    "        click_history,\n",
    "        positive_samples,\n",
    "        negative_samples\n",
    "    ])\n",
    "\n",
    "with open('val.txt', 'wb') as f:\n",
    "    pickle.dump(new_val_data, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T07:42:29.796037Z",
     "start_time": "2024-06-18T07:42:25.208479Z"
    }
   },
   "id": "414242a1aec5c096"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#making a test subset from validation data\n",
    "subset_val_df = val_behaviors_df[~val_behaviors_df['impression_id'].isin(new_val_data[0])]\n",
    "test_df = subset_val_df.sample(n=73152, random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d868c5d1c5044282"
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "new_test_data = [ [],[],[], [] ]\n",
    "\n",
    "for index, row in test_df.iterrows():\n",
    "    new_test_data[0].append(row['impression_id'])\n",
    "    new_test_data[1].append(str(row['user_id']))\n",
    "    new_test_data[2].append(str(row['impression_time']))\n",
    "    \n",
    "    clicked_articles = set(row['article_ids_clicked']) if len(row['article_ids_clicked']) > 0 else set()\n",
    "    inview_articles = set(row['article_ids_inview']) if len(row['article_ids_inview']) > 0 else set()\n",
    "    positive_samples = [str(article) for article in clicked_articles] \n",
    "    negative_samples = [str(article) for article in (inview_articles - clicked_articles)]\n",
    "    click_history = get_users_click_history(row['user_id'], row['impression_time'], val_history_df)\n",
    "    new_test_data[3].append([\n",
    "        click_history,\n",
    "        positive_samples,\n",
    "        negative_samples\n",
    "    ])\n",
    "\n",
    "with open('test.txt', 'wb') as f:\n",
    "    pickle.dump(new_test_data, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T08:25:58.302607Z",
     "start_time": "2024-06-18T08:25:19.837148Z"
    }
   },
   "id": "cbfc90f3cc3831cf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Function to create dictionary mapping users to histories (for clustering data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38f8fdbb32d2c8ac"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_users = new_train_data[1]\n",
    "all_users.extend(new_val_data[1])\n",
    "all_users.extend(new_test_data[1])\n",
    "\n",
    "all_hists = []\n",
    "for i in new_train_data[3]:\n",
    "    ids = i[0]\n",
    "    all_hists.append(ids)\n",
    "    \n",
    "for i in new_val_data[3]:\n",
    "    ids = i[0]\n",
    "    all_hists.append(ids)\n",
    "    \n",
    "for i in new_test_data[3]:\n",
    "    ids = i[0]\n",
    "    all_hists.append(ids)\n",
    "    \n",
    "def create_user_history(user_ids, article_id_lists):\n",
    "    user_history = {}\n",
    "    \n",
    "    for user_id, article_ids in zip(user_ids, article_id_lists):\n",
    "        if user_id not in user_history:\n",
    "            user_history[user_id] = set()\n",
    "        user_history[user_id].update(article_ids)\n",
    "    \n",
    "    return user_history\n",
    "\n",
    "users_arts = create_user_history(all_users, all_hists)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f041d30e349012de"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create user-article information from user-history dictionary for clustering"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5aab6c3abaf8de0a"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def create_user_article_info(user_history, articles_df):\n",
    "\n",
    "    articles_df = articles_df.reset_index(drop=True)\n",
    "    articles_dict = articles_df.set_index('title').T.to_dict()\n",
    "\n",
    "    user_info = {}\n",
    "    total_users = len(user_history)\n",
    "    for user_counter, (user_id, article_ids) in enumerate(user_history.items(), 1):\n",
    "        user_info[user_id] = {\n",
    "            'topics': [],\n",
    "            'sentiment': [],\n",
    "            'page_views': [],\n",
    "            'time_published': [],\n",
    "            'article_ids': []\n",
    "        }\n",
    "        \n",
    "        for article_id in article_ids:\n",
    "            title = new_dataset_dict[article_id][\"title\"]\n",
    "            \n",
    "            if title in articles_dict:\n",
    "                article_data = articles_dict[title]\n",
    "                user_info[user_id]['topics'].append(article_data['topics'])\n",
    "                user_info[user_id]['sentiment'].append(article_data['sentiment_score'])\n",
    "                user_info[user_id]['page_views'].append(article_data['total_pageviews'])\n",
    "                user_info[user_id]['time_published'].append(article_data['published_time'])\n",
    "                user_info[user_id]['article_ids'].append(article_id)\n",
    "        \n",
    "        user_progress = (user_counter / total_users) * 100\n",
    "        print(f\"Overall progress: {user_progress:.2f}%\")\n",
    "    \n",
    "    return user_info\n",
    "\n",
    "user_article_info = create_user_article_info(users_arts, articles_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T17:04:10.098709Z",
     "start_time": "2024-06-28T17:04:10.095780Z"
    }
   },
   "id": "93e4ee3fd844c6c8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#adding info about index of each articleIDs info in the data for clustering\n",
    "def transform_data(user_data):\n",
    "    transformed_data = {}\n",
    "    \n",
    "    for user_id, data in user_data.items():\n",
    "        new_data = data.copy()  \n",
    "        article_ids = new_data.pop('article_ids', [])\n",
    "        index_dict = {index: article_id for index, article_id in enumerate(article_ids)}\n",
    "        new_data['index'] = index_dict\n",
    "        transformed_data[user_id] = new_data\n",
    "    return transformed_data\n",
    "newewst_dict = transform_data(user_article_info)\n",
    "with open('final_cluster_data.txt', 'wb') as handle:\n",
    "    pickle.dump(newewst_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69ca5b10e18aea63"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
