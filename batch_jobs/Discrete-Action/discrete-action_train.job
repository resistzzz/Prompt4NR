#!/bin/bash
#SBATCH --partition=gpu
#SBATCH --gpus=4
#SBATCH --job-name=DiscreteActionTrain
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=18
#SBATCH --time=06:30:00
#SBATCH --output=discrete-action_train_output_%A.out

module purge
module load 2022
module load Anaconda3/2022.05

# Activate environment
source activate recsys

# Path of framework
FW=$HOME/Prompt4NR
TEMPLATE=$FW/Discrete-Action

# Change directory to the model template
cd $TEMPLATE

# Copy datasets to temp folder to save read/write operations computational costs
export DATA_SET=/large_full_balanced

if [ ! -d "$TMPDIR$DATA_SET" ]; then
    mkdir $TMPDIR$DATA_SET
fi

rsync -a $FW/DATA$DATA_SET/ $TMPDIR$DATA_SET

# Check whether the GPU is available
# srun python -uc "import torch; print('GPU available?', torch.cuda.is_available())"

# Model to use
MODEL_NAME=bert-base-multilingual-uncased

# Run python scripts that trains the model. Content of run_py3.sh
srun python3 -u main-multigpu.py --data_path $TMPDIR$DATA_SET --model_name $MODEL_NAME --epochs 3 --batch_size 16 --test_batch_size 100 --wd 1e-3 --max_tokens 500 --log True --world_size 4 --model_save True
